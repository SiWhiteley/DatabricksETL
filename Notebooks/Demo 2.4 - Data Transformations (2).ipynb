{"cells":[{"cell_type":"code","source":["#Gather relevant keys from our Secret Scope\nServicePrincipalID = dbutils.secrets.get(scope = \"Analysts\", key = \"SPID\")\nServicePrincipalKey = dbutils.secrets.get(scope = \"Analysts\", key = \"SPKey\")\nDirectoryID = dbutils.secrets.get(scope = \"Analysts\", key = \"DirectoryID\")\nDBUser = dbutils.secrets.get(scope = \"Analysts\", key = \"DBUser\")\nDBPassword = dbutils.secrets.get(scope = \"Analysts\", key = \"DBPword\")\n\n\n#Combine DirectoryID into full string\nDirectory = \"https://login.microsoftonline.com/{}/oauth2/token\".format(DirectoryID)\n\n#Configure our ADLS Gen 2 connection with our service principal details\nspark.conf.set(\"fs.azure.account.auth.type\", \"OAuth\")\nspark.conf.set(\"fs.azure.account.oauth.provider.type\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\nspark.conf.set(\"fs.azure.account.oauth2.client.id\", ServicePrincipalID)\nspark.conf.set(\"fs.azure.account.oauth2.client.secret\", ServicePrincipalKey)\nspark.conf.set(\"fs.azure.account.oauth2.client.endpoint\", Directory)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"markdown","source":["### Data Transformations\nThe kind of transformations we might do here are exactly those that we would do in any other ETL system. We can add calculated columns, strip out columns we don't need, rename columns to be more use friendly and lookup reference data"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\n# Define the schema over the Taxi data we're going to be bringing in\nfactSchema = StructType([\n  StructField(\"Dispatching_base_num\", StringType(), True),\n  StructField(\"Pickup_Datetime\", TimestampType(), True),\n  StructField(\"DropOff_datetime\", TimestampType(), True),\n  StructField(\"PULocationID\", IntegerType(), True),\n  StructField(\"DOlocationID\", IntegerType(), True)])\n\n# Define the Taxi data frame\nfactdf = (spark\n     .read\n     .option(\"header\",\"true\")\n     .schema(factSchema)\n     .csv(\"/mnt/taxi/taxiFull/SmallSlice.csv\")\n     )\n\n# Define the Lookup data frame over our cleaned Taxi Zone data\nlookupdf = (spark\n       .read\n       .parquet(\"abfss://root@dblake.dfs.core.windows.net/BASE/Public/TaxiZones/v1/parquet/\")\n     )"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["We now have a very large dataset with some IDs, and a lookup file that provides more information about those IDs. In this case, we're preparing a file for further analysis, so we want to denormalise those useful lookup attributes onto the fact table so we can query it as one, efficient dataset.\n\nFirst, let's lookup the Pickup Location - we'll use an inner join which will trim down our data somewhat, but we're only interested in those rides where we have this information\n\nOnce we've joined the DataFrames, we'll have a new structure that contains the superset of colunmns. We'll do some column renaming to make it clear where those columns came from"],"metadata":{}},{"cell_type":"code","source":["# Define a new dataframe using a default (inner) join between the fact DataFrame and our lookup DataFrame\njoindf = (factdf\n           .join(lookupdf, factdf[\"PULocationID\"] == lookupdf[\"LocationID\"])\n         )\n\n# Tidy up the DataFrame, renaming columns to show they're from the Pickup Location\njoindf = (joindf\n           .drop(\"LocationID\")\n           .withColumnRenamed(\"Borough\",\"PickupBorough\")\n           .withColumnRenamed(\"Zone\",\"PickupZone\")\n           .withColumnRenamed(\"service_zone\",\"PickupServiceZone\")\n         )"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["Ok, we've added some details around the pickup location. Now let's look at the DropOff location. This is a much sparser populated field and so we should use a left outer join, much as we would in SQL."],"metadata":{}},{"cell_type":"code","source":["# Create a new DataFrame performing the left outer join\nfulldf = (joindf\n           .join(lookupdf, joindf[\"DOLocationID\"] == lookupdf[\"LocationID\"], \"leftouter\")\n         )\n\n# Tidy up the DataFrame once more\nfulldf = (fulldf\n           .drop(\"LocationID\")\n           .withColumnRenamed(\"Borough\",\"DropOffBorough\")\n           .withColumnRenamed(\"Zone\",\"DropOffZone\")\n           .withColumnRenamed(\"service_zone\",\"DropOffServiceZone\")\n         )"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["# Let's review our results\ndisplay(fulldf.limit(100))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Dispatching_base_num</th><th>Pickup_Datetime</th><th>DropOff_datetime</th><th>PULocationID</th><th>DOlocationID</th><th>PickupBorough</th><th>PickupZone</th><th>PickupServiceZone</th><th>DropOffBorough</th><th>DropOffZone</th><th>DropOffServiceZone</th></tr></thead><tbody><tr><td>B00029</td><td>2017-01-01T00:22:00.000+0000</td><td>null</td><td>3</td><td>null</td><td>Bronx</td><td>Allerton/Pelham Gardens</td><td>Boro Zone</td><td>null</td><td>null</td><td>null</td></tr><tr><td>B00029</td><td>2017-01-01T00:01:00.000+0000</td><td>null</td><td>3</td><td>null</td><td>Bronx</td><td>Allerton/Pelham Gardens</td><td>Boro Zone</td><td>null</td><td>null</td><td>null</td></tr><tr><td>B00029</td><td>2017-01-01T00:16:00.000+0000</td><td>null</td><td>51</td><td>null</td><td>Bronx</td><td>Co-Op City</td><td>Boro Zone</td><td>null</td><td>null</td><td>null</td></tr><tr><td>B00029</td><td>2017-01-01T00:46:00.000+0000</td><td>null</td><td>185</td><td>null</td><td>Bronx</td><td>Pelham Parkway</td><td>Boro Zone</td><td>null</td><td>null</td><td>null</td></tr><tr><td>B00029</td><td>2017-01-01T00:56:00.000+0000</td><td>null</td><td>174</td><td>null</td><td>Bronx</td><td>Norwood</td><td>Boro Zone</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"]}}],"execution_count":8}],"metadata":{"name":"Demo 2.4 - Data Transformations","notebookId":1223194938974018},"nbformat":4,"nbformat_minor":0}
