{"cells":[{"cell_type":"markdown","source":["<div style=\"text-align: left; line-height: 0; padding-top: 9px; padding-left:150px\">\n  <img src=\"https://static1.squarespace.com/static/5bce4071ab1a620db382773e/t/5d266c78abb6d10001e4013e/1562799225083/appliedazuredatabricks3.png\" alt=\"Databricks Learning\" style=\"width: 600px; height: 163px\">\n</div>\n# *Applied Azure Databricks*. Presented by <a href=\"www.advancinganalytics.co.uk\">Advancing Analytics</a>\nCourse delivered by Simon Whiteley - <a href=\"mailto:simon@advancinganalytics.co.uk\">simon@advancinganalytics.co.uk</a>"],"metadata":{}},{"cell_type":"markdown","source":["## Widgets\nFist things first - we need to have some parameters, so let's create a widget"],"metadata":{}},{"cell_type":"code","source":["dbutils.widgets.removeAll()\ndbutils.widgets.text(\"fileName\", \"Product\",\"AdventureWorks Table\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["### Read current widget value\nWe can now use dbutils.widgets to get the current value of our widget parameter"],"metadata":{}},{"cell_type":"code","source":["fileName = dbutils.widgets.get(\"fileName\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["### Read the schema json for our selected file\nI've stored a schema file for each of the data files in my lake. I can pick up the right file for the dataset selected by my widget"],"metadata":{}},{"cell_type":"code","source":["#Load the relevant libraries to build schemas and read JSON\nfrom pyspark.sql.types import *\nimport json\n\n#Inject our filename into the lake path\nschemaLocation = f\"/mnt/dblake/RAW/Public/Adventureworks/SalesLT.{fileName}.json\"\n\n#Read the json file contents\njschemadf = sqlContext.read.text(schemaLocation)\n\n#Pull out the first value (it's all one value but the reader turns it into a dataframe)\njschema = jschemadf.first().value\n\n#Convert our JSON schema into a pyspark Struct which can be applied directly to a dataframe\nnewSchema = StructType.fromJson(json.loads(jschema))\nnewSchema"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[44]: StructType(List(StructField(CustomerID,IntegerType,true),StructField(NameStyle,BooleanType,true),StructField(Title,StringType,true),StructField(FirstName,StringType,true),StructField(MiddleName,StringType,true),StructField(LastName,StringType,true),StructField(Suffix,StringType,true),StructField(CompanyName,StringType,true),StructField(SalesPerson,StringType,true),StructField(EmailAddress,StringType,true),StructField(Phone,StringType,true),StructField(PasswordHash,StringType,true),StructField(PasswordSalt,StringType,true),StructField(rowguid,StringType,true),StructField(ModifiedDate,StringType,true)))</div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["### We have a schema, now we need to create a dataframe\nWe can derive the path of our dataset in the same way as we did with the schema. We then combine schema and data location in a new dataframe\n\nWe're also going to use \"_corrupt_record\", this is a system field which will only be populated if a row fails to parse into the structure we've provided"],"metadata":{}},{"cell_type":"code","source":["#Inject the filename into our lake path for the dataset\ndataLocation = f\"/mnt/dblake/RAW/Public/Adventureworks/SalesLT.{fileName}/\"\n\n#Add some magic - add a new column to our structure for the _corrupt_record system field\nnewSchema.add(\"_corrupt_record\", StringType(), True)\n\n# Now let's load the data, allowing any malformed records to come through but populating our corrupt_record field\ndf = (spark\n       .read\n       .schema(newSchema)\n       .option(\"badRecordsPath\", f\"{dataLocation}/_reject\")\n       .csv(dataLocation)\n     )"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["### Write the Good rows to a Parquet directory\nParquet compresses a lot better than CSV, and it has the schema built in. It is a STRUCTURED file type"],"metadata":{}},{"cell_type":"code","source":["outputLocation = f\"/mnt/dblake/BASE/Public/Adventureworks/SalesLT.{fileName}/\"\ndf.write.mode('overwrite').format(\"parquet\").save(outputLocation)\n\nspark.sql(f\"create table if not exists Denmark2020.{fileName} using PARQUET location '{outputLocation}'\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[50]: DataFrame[]</div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["###Return results back to the parent caller\ndbutils.notebook.exit() returns a message to whatever called the notebook - this can send a message back to ADF!"],"metadata":{}},{"cell_type":"code","source":["processedRows = (spark.read.csv(dataLocation)).count()\ngoodRows = df.count()\nrejectedRows = processedRows - goodRows\n\ndbutils.notebook.exit(json.dumps({\"processedRows\":processedRows, \"goodRows\":goodRows, \"rejectedRows\":rejectedRows, \"status\":\"Succeeded\"}))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/plain":["{\"processedRows\": 848, \"goodRows\": 848, \"rejectedRows\": 0, \"status\": \"Succeeded\"}"]}}],"execution_count":13}],"metadata":{"name":"Dynamic Validation","notebookId":482375737238041},"nbformat":4,"nbformat_minor":0}
