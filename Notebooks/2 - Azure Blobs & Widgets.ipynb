{"cells":[{"cell_type":"markdown","source":["This notebook shows how to create and query a table or DataFrame on Azure Blob Storage."],"metadata":{}},{"cell_type":"markdown","source":["### Step 1: Set the data location and type\n\nThere are two ways in Databricks to read from Azure Blob Storage. You can either read data using [account keys](https://docs.databricks.com/spark/latest/data-sources/azure/azure-storage.html#azure-blob-storage) or read data using shared access signatures (SAS).\n\nTo get started, we need to set the location and type of the file. We can do this using [widgets](https://docs.databricks.com/user-guide/notebooks/widgets.html). Widgets allow us to parameterize the exectuion of this entire notebook. First we create them, then we can reference them throughout the notebook."],"metadata":{}},{"cell_type":"code","source":["dbutils.widgets.text(\"storage_account_name\", \"sawstaging\", \"Storage Account Name\")\ndbutils.widgets.text(\"storage_account_access_key\", \"daU2wY2rntDb7EblyVoV5CMG1y2wRKDjrfWgneoS8z0Km1Uzc3Ykjcxi/kwwP6yeclCEsQzz+OfQDa4eWKdOhw==\", \"Storage Access Key / SAS\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["dbutils.widgets.text(\"file_location\", \"wasbs://example/location\", \"Upload Location\")\ndbutils.widgets.dropdown(\"file_type\", \"csv\", [\"csv\", 'parquet', 'json'])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["spark.conf.set(\n  \"fs.azure.account.key.\"+dbutils.widgets.get(\"storage_account_name\")+\".blob.core.windows.net\",\n  dbutils.widgets.get(\"storage_account_access_key\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["### Step 2: Read the data\n\nNow that we have specified our file metadata, we can create a DataFrame. Notice that we use an *option* to specify that we want to infer the schema from the file. We can also explicitly set this to a particular schema if we have one already.\n\nFirst, let's create a DataFrame in Python."],"metadata":{}},{"cell_type":"code","source":["df = spark.read.format(dbutils.widgets.get(\"file_type\")).option(\"inferSchema\", \"true\").load(dbutils.widgets.get(\"file_location\"))"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["### Step 3: Query the data\n\nNow that we have created our DataFrame, we can query it. For instance, you can identify particular columns to select and display within Databricks."],"metadata":{}},{"cell_type":"code","source":["display(df.select(\"EXAMPLE_COLUMN\"))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["### Step 4: (Optional) Create a view or table\n\nIf you want to query this data as a table, you can simply register it as a *view* or a table."],"metadata":{}},{"cell_type":"code","source":["df.createOrReplaceTempView(\"YOUR_TEMP_VIEW_NAME\")"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["We can query this view using Spark SQL. For instance, we can perform a simple aggregation. Notice how we can use `%sql` to query the view from SQL."],"metadata":{}},{"cell_type":"code","source":["%sql\n\nSELECT EXAMPLE_GROUP, SUM(EXAMPLE_AGG) FROM YOUR_TEMP_VIEW_NAME GROUP BY EXAMPLE_GROUP"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Registered as a temp view, this data is only available to this particular notebook. If you want other users to be able to query this table, you can also create a table from the DataFrame."],"metadata":{}},{"cell_type":"code","source":["df.write.format(\"parquet\").saveAsTable(\"MY_PERMANENT_TABLE_NAME\")"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["This table will persist across cluster restarts and allow various users across different notebooks to query this data."],"metadata":{}}],"metadata":{"name":"2 - Azure Blobs & Widgets","notebookId":3643594848502900},"nbformat":4,"nbformat_minor":0}
